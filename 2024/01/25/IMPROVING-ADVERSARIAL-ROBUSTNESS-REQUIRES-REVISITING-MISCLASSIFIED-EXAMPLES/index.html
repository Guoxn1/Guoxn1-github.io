

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Guoxin">
  <meta name="keywords" content="">
  
    <meta name="description" content="0 摘要 深度神经网络（DNN）很容易受到由不可感知的扰动制作的对抗性示例的影响。已经提出了一系列防御技术来提高DNN对对抗性示例的鲁棒性，其中对抗性训练已被证明是最有效的。对抗性训练通常被表述为一个最小-最大优化问题，内部最大化用于生成对抗性示例。然而，存在一个简单但容易被忽视的事实，即对抗性示例只定义在正确分类的（自然）示例上，但不可避免地，一些（自然）示例在训练过程中会被错误分类。在本文中">
<meta property="og:type" content="article">
<meta property="og:title" content="IMPROVING ADVERSARIAL ROBUSTNESS REQUIRES REVISITING MISCLASSIFIED EXAMPLES">
<meta property="og:url" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="0 摘要 深度神经网络（DNN）很容易受到由不可感知的扰动制作的对抗性示例的影响。已经提出了一系列防御技术来提高DNN对对抗性示例的鲁棒性，其中对抗性训练已被证明是最有效的。对抗性训练通常被表述为一个最小-最大优化问题，内部最大化用于生成对抗性示例。然而，存在一个简单但容易被忽视的事实，即对抗性示例只定义在正确分类的（自然）示例上，但不可避免地，一些（自然）示例在训练过程中会被错误分类。在本文中">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125101449117.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125102345557.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125102701156.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125103655916.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125135859802.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125145702975.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125194206114.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125194249827.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125194316033.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125200641848.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125200941922.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125234407211.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125200941922.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125204355981.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125204643929.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125211407518.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125211526831.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125211541264.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125211738884.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125214955611.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125215727227.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125215950845.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125220046653.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125220157874.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125233252737.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125233456732.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125234045278.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125234122216.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240126090717897.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240126090724544.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240126091031406.png">
<meta property="og:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240126091142848.png">
<meta property="article:published_time" content="2024-01-25T01:15:44.000Z">
<meta property="article:modified_time" content="2024-01-28T01:20:13.283Z">
<meta property="article:author" content="Guoxin">
<meta property="article:tag" content="论文读后总结">
<meta property="article:tag" content="对抗样本">
<meta property="article:tag" content="对抗训练">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/image-20240125101449117.png">
  
  
  
  <title>IMPROVING ADVERSARIAL ROBUSTNESS REQUIRES REVISITING MISCLASSIFIED EXAMPLES - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Guoxin</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="IMPROVING ADVERSARIAL ROBUSTNESS REQUIRES REVISITING MISCLASSIFIED EXAMPLES"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-01-25 09:15" pubdate>
          2024年1月25日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          11k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          91 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">IMPROVING ADVERSARIAL ROBUSTNESS REQUIRES REVISITING MISCLASSIFIED EXAMPLES</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="摘要">0 摘要</h1>
<p>深度神经网络（DNN）很容易受到由不可感知的扰动制作的对抗性示例的影响。已经提出了一系列防御技术来提高DNN对对抗性示例的鲁棒性，其中对抗性训练已被证明是最有效的。对抗性训练通常被表述为一个最小-最大优化问题，内部最大化用于生成对抗性示例。然而，存在一个简单但容易被忽视的事实，即对抗性示例只定义在正确分类的（自然）示例上，但不可避免地，一些（自然）示例在训练过程中会被错误分类。在本文中，我们研究了错误分类和正确分类的示例对对抗训练的最终鲁棒性的独特影响。具体来说，我们发现错误分类的例子确实对最终的鲁棒性有重大影响。更令人惊讶的是，我们发现不同的最大化技术在错误分类的例子中可能对最终的鲁棒性有微不足道的影响，而不同的最小化技术是至关重要的。我们提出了一种新的防御算法，称为错误分类感知对抗训练（MART），它在训练过程中明确区分错误分类和正确分类的示例。我们还提出了一个半监督的扩展MART，它可以利用未标记的数据，以进一步提高鲁棒性。实验结果表明，MART及其变体可以显着提高最先进的对抗鲁棒性。</p>
<h1 id="结论">1 结论</h1>
<p>在本文中，我们研究了一个有趣的观察结果，即错误分类的示例对对抗训练的最终鲁棒性有明显的影响，特别是对于外部最小化过程。基于这一观察，我们设计了一个误分类感知对抗风险，它被公式化为在标准对抗风险中添加一个误分类感知正则化。在正则化对抗风险之后，我们提出了一种新的防御算法，称为错误分类感知对抗训练（MART），并具有适当的代理损失函数。实验结果表明，MART可以实现相对于最先进的对抗鲁棒性显着提高，也可以实现更好的鲁棒性与额外的未标记的数据。</p>
<h1 id="介绍">2 介绍</h1>
<p>与预处理/后处理方法(如特征压缩(Xu et al.， 2017))相比，输入去噪(Guo
et al.，
2018;廖等人，2018;Samangouei等人，2018;Bai等人，2019)和对抗检测(Feinman等人，2017;Ma
et al.，
2018;Lee等人，2018年)提出了几种防御技术来训练dnn，这些dnn对对抗例子具有固有的鲁棒性，包括防御蒸馏(Papernot等人，2016年)、梯度正则化(Gu
&amp; Rigazio, 2014年;Papernot et al.， 2017;Ross &amp; Doshi-Velez,
2018年;Tramèr等人，2018)，模型压缩(Das等人，2018;Liu等人，2018)和激活剪枝(Dhillon等人，2018;其中对抗训练已被证明是最有效的(Athalye
et al.，
2018)。对抗训练可以看作是一种基于对抗例子训练dnn的数据增强技术，可以看作是解决以下最小最大化优化问题(Madry
et al.， 2018):</p>
<figure>
<img src="image-20240125101449117.png" srcset="/img/loading.gif" lazyload alt="image-20240125101449117">
<figcaption aria-hidden="true">image-20240125101449117</figcaption>
</figure>
<p>n为训练样本个数，(·)为分类损失，如常用的交叉熵(cross entropy,
CE)损失。内部最大化生成对抗实例，外部最小化可用于训练鲁棒dnn。</p>
<p>大多数对抗性训练变体忽略了这一区别，无论它们是否被正确分类，所有的训练示例在最大化和最小化过程中都被同等对待。我们知道的唯一例外是Ding等人(2018)，他们提出对正确分类的例子使用最大边际优化。然而，他们没有对误分类的例子给予足够的重视。文献中对于错误分类和正确分类的例子对鲁棒性的影响仍缺乏更深入的理解。因此，我们提出以下问题:</p>
<p>对抗例子是否由i)错误分类和ii)正确分类的例子生成，对对抗鲁棒性同样重要?如果不是，我们如何更好地利用这种差异来提高稳健性?</p>
<p>在本文中，我们研究了对抗训练中这个有趣但迄今被忽视的方面，并发现错误分类和正确分类的例子对最终的鲁棒性有显著的影响。为了说明这一现象，我们在白盒环境中对CIFAR10进行了一个概念证明实验，L∞最大扰动=
8/255。我们首先使用10步PGD
(PGD10)标准对抗训练训练一个8层卷积神经网络(CNN)，步长/4，然后使用该网络(训练精度87%)选择两个自然训练示例子集进行研究:1)误分类样本的子集S−(占训练数据的13%)，2)正确分类样本的子集S+(同样占训练数据的13%，|S+|
=
|S−|)。利用这两个子集，我们探索了不同的方法来重新训练同一个网络，并评估其对测试数据集上的白盒PGD20(步长/10)攻击的鲁棒性。</p>
<p>在图1(a)中，我们发现误分类的例子对最终的鲁棒性有显著的影响。与标准对抗训练(蓝虚线)相比，如果子集S−中的样本在对抗训练过程中不受干扰(绿实线)(其他样本仍受PGD10干扰)，最终鲁棒性会急剧下降。相比之下，对子集S+进行同样的操作，对最终稳健性的影响很小(实橙色线)。之前的研究发现，去除一小部分训练示例并不会降低鲁棒性(Ding
et al.，
2019)，这对于正确分类的示例似乎是正确的，但对于错误分类的示例显然不是正确的。</p>
<figure>
<img src="image-20240125102345557.png" srcset="/img/loading.gif" lazyload alt="image-20240125102345557">
<figcaption aria-hidden="true">image-20240125102345557</figcaption>
</figure>
<p>为了进一步理解错误分类和正确分类的不同影响，我们在对抗训练的最大化或最小化过程中测试了不同的技术。首先，在保持最小损耗CE不变的情况下，我们采用了不同的最大化技术。如图1(b)所示，当我们使用弱攻击(如Fast
Gradient Sign Method (FGSM) (Goodfellow et al.，
2015))去扰动误分类的例子S−时(所有其他训练例子仍然受到PGD10的扰动)，最终的鲁棒性几乎没有受到影响。这表明，如果内最大化问题的求解精度适中，则对误分类样本S−的不同最大化技术对最终鲁棒性的影响可以忽略不计(Wang
et al.， 2019)。然而，对于子集S+，对于最大化的弱攻击会使鲁棒性退化。</p>
<figure>
<img src="image-20240125102701156.png" srcset="/img/loading.gif" lazyload alt="image-20240125102701156">
<figcaption aria-hidden="true">image-20240125102701156</figcaption>
</figure>
<p>其次，我们测试了不同的最小化技术，内部最大化仍然由PGD10解决。有趣的是，我们发现对错误分类的例子采用不同的最小化技术会对最终的鲁棒性产生显著的影响。如图1(c)所示，与CE损耗的标准对抗训练(蓝虚线)相比，当错误分类示例的外部最小化被“正则化”(绿实线)并增加一项(之前Zheng等人(2016)使用的KL-divergence项)时，最终的鲁棒性显著提高;张等(2019))。同样的正则化应用于正确分类的例子，也有助于最终的鲁棒性(实心橙色线)，尽管不如对错误分类的例子显著。</p>
<figure>
<img src="image-20240125103655916.png" srcset="/img/loading.gif" lazyload alt="image-20240125103655916">
<figcaption aria-hidden="true">image-20240125103655916</figcaption>
</figure>
<p>在上述观察的激励下，我们重新制定对抗威胁，以正则化的形式结合了对错误分类例子的明确区分。然后，我们提出了一种新的防御算法，以在对抗训练期间以动态方式实现这一目标。我们的主要贡献是：</p>
<p>1
我们研究了错误分类和正确分类的例子对对抗训练的最终鲁棒性的独特影响。我们发现，对错误分类的例子有更多的操作对最终的鲁棒性的影响，最小化技术比最大化技术在最小-最大优化框架下更重要。</p>
<p>2
我们提出了一个正则化的对抗威胁，它包含了一个明确的分类错误的例子作为正则化。在此基础上，我们进一步提出了一种新的防御算法，称为误分类感知对抗训练（MART）。</p>
<p>3
在实验中，我们表明对抗鲁棒性可以通过特别关注错误分类的示例而显着提高。它还有助于改进最近提出的使用未标记数据的对抗训练。</p>
<h1 id="误分类意识对抗威胁">3 误分类意识对抗威胁</h1>
<p>在本节中，我们提出了一个正则化的对抗威胁，它包含了对错误分类示例的显式区分。</p>
<h2 id="准备工作">3.1 准备工作</h2>
<p>对于K类（K ≥ 2）分类问题，给定数据集{（Xi，yi）}i=1，.，n，其中Xi ∈
Rd作为自然例子，yi ∈
{1，...，K}作为其相关联的标签，具有模型参数θ的DNN分类器hθ预测输入示例Xi的类别：</p>
<figure>
<img src="image-20240125135859802.png" srcset="/img/loading.gif" lazyload alt="image-20240125135859802">
<figcaption aria-hidden="true">image-20240125135859802</figcaption>
</figure>
<p>其中zk（Xi，θ）是网络相对于类别k的logits输出，pk（Xi，θ）是Xi属于类别k的概率（softmax
on logits）。</p>
<p>对抗风险可以描述如下情况：</p>
<figure>
<img src="image-20240125145702975.png" srcset="/img/loading.gif" lazyload alt="image-20240125145702975">
<figcaption aria-hidden="true">image-20240125145702975</figcaption>
</figure>
<p>其中1是判别函数。</p>
<h2 id="误分类意识正则">3.2 误分类意识正则</h2>
<p>我们根据对当前网络hθ的预测重新制定了对抗风险。具体来说，自然训练样本可以相对于hθ分为两个子集，一个子集是正确分类的样本（S
+hθ），另一个子集是错误分类的样本（S− hθ）：</p>
<figure>
<img src="image-20240125194206114.png" srcset="/img/loading.gif" lazyload alt="image-20240125194206114">
<figcaption aria-hidden="true">image-20240125194206114</figcaption>
</figure>
<p>然后，我们将分别为正确分类和错误分类的示例定义对抗风险。正如我们在图1（c）中所观察到的，对错误分类的示例进行正则化可以显着提高鲁棒性。因此，对于错误分类的示例，我们将对抗风险公式化为：</p>
<figure>
<img src="image-20240125194249827.png" srcset="/img/loading.gif" lazyload alt="image-20240125194249827">
<figcaption aria-hidden="true">image-20240125194249827</figcaption>
</figure>
<p>xi就是通过求解下面的式子得出的：</p>
<figure>
<img src="image-20240125194316033.png" srcset="/img/loading.gif" lazyload alt="image-20240125194316033">
<figcaption aria-hidden="true">image-20240125194316033</figcaption>
</figure>
<p>我们注意到R.H.S.上的第一项和第二项。（4）分别对应于标准对抗风险和正则化项。此外，我们想澄清的是，正则化项1（hθ（Xi）=
hθ（x
i））旨在鼓励神经网络的输出在错误分类的对抗性样本中保持稳定。对于错误分类的例子，直接最小化标准对抗风险可能太难了，因为即使没有任何扰动，它们也不能正确分类。</p>
<p>对正确分类的例子进行正则化不能像对错误分类的例子那样提供显著的改进。此外，在这种情况下，可以发现1（hθ（Xi）&lt;<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="31.378ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 13869.1 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(1055.8,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"/></g><g data-mml-node="mi" transform="translate(1631.8,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"/></g><g data-mml-node="mi" transform="translate(2100.8,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">（</text></g><g data-mml-node="mi" transform="translate(3100.8,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mi" transform="translate(3672.8,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(4017.8,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">）</text></g><g data-mml-node="mi" transform="translate(5017.8,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">）</text></g><g data-mml-node="mo" transform="translate(6295.6,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(7351.3,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mi" transform="translate(7851.3,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">（</text></g><g data-mml-node="mi" transform="translate(8851.3,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"/></g><g data-mml-node="mi" transform="translate(9427.3,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"/></g><g data-mml-node="mi" transform="translate(9896.3,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">（</text></g><g data-mml-node="mi" transform="translate(10896.3,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mi" transform="translate(11468.3,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(11813.3,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">）</text></g><g data-mml-node="mo" transform="translate(13091.1,0)"><path data-c="3C" d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z"/></g></g></g></svg></mjx-container></span>=
yi），因为我们有hθ（xi）=
yi，这意味着正则化子与对抗风险具有完全相同的形式。因此，对于正确分类的示例，我们简单地使用标准对抗风险，即</p>
<figure>
<img src="image-20240125200641848.png" srcset="/img/loading.gif" lazyload alt="image-20240125200641848">
<figcaption aria-hidden="true">image-20240125200641848</figcaption>
</figure>
<p>最后，在对抗性训练框架中结合正确分类的示例和错误分类的示例的两种对抗性风险，我们可以训练一个最小化以下风险的网络：</p>
<figure>
<img src="image-20240125200941922.png" srcset="/img/loading.gif" lazyload alt="image-20240125200941922">
<figcaption aria-hidden="true">image-20240125200941922</figcaption>
</figure>
<p>上面定义的新风险是具有正则化项的正则化对抗风险，我们称之为误分类感知正则化。</p>
<p>注意到，这里变成了R-变成了相乘而上面是相加。这里主要用到了求和的化简。</p>
<h1 id="拟议防御措施误分类意识不良培训mart">4
拟议防御措施：误分类意识不良培训（MART）</h1>
<p>在上一节中，我们推导了基于0-1损失的误分类感知对抗风险。然而，0-1损失的优化在实践中是棘手的。接下来，我们提出了一个误分类感知的adveRsarial训练（MART）算法，通过替换0-1损失与适当的代理损失函数，这是物理上有意义的和计算上容易处理的。在此基础上，我们进一步分析了MART与现有工作的区别，并提出了一种半监督的扩展。</p>
<figure>
<img src="image-20240125234407211.png" srcset="/img/loading.gif" lazyload alt="image-20240125234407211">
<figcaption aria-hidden="true">image-20240125234407211</figcaption>
</figure>
<h2 id="提出的防御算法">4.1 提出的防御算法</h2>
<p>外部最小化的替代损失。最主要7中的就是三个函数。</p>
<figure>
<img src="image-20240125200941922.png" srcset="/img/loading.gif" lazyload alt="image-20240125200941922">
<figcaption aria-hidden="true">image-20240125200941922</figcaption>
</figure>
<p>对于第一个判别函数，我们提出使用增强交叉熵（BCE）损失作为替代损失。这在很大程度上是因为对对抗性样本进行分类需要比自然样本更强的分类器，因为对抗性样本的存在使分类决策边界变得更加复杂。</p>
<figure>
<img src="image-20240125204355981.png" srcset="/img/loading.gif" lazyload alt="image-20240125204355981">
<figcaption aria-hidden="true">image-20240125204355981</figcaption>
</figure>
<p>其中pk（x i，θ）是在（2）中定义的概率输出，第一项−log pyi（x
i，θ）是常用的CE损失，表示为CE（p（x i，θ），yi），第二项−log 1 − maxk
=yi pk（x i，θ）是用于提高分类器的改善分类器的决策边界。</p>
<p>对于第二个指标函数，我们可以使用KL散度作为替代损失函数。因为hθ（Xi）!=
hθ（x i‘）意味着对抗样本的输出分布与自然样本的输出分布不同。</p>
<figure>
<img src="image-20240125204643929.png" srcset="/img/loading.gif" lazyload alt="image-20240125204643929">
<figcaption aria-hidden="true">image-20240125204643929</figcaption>
</figure>
<p>第三指示函数1（hθ（Xi）xi =
yi）是强调对错误分类的示例进行学习的条件。然而，如果我们在训练过程中进行硬决策，则无法直接优化条件。我们建议使用软判决方案，用输出概率1
− pyi（Xi，θ）替换1（hθ（Xi）n =
yi）。对于错误分类的示例，这将很大，而对于正确分类的示例，这将很小。</p>
<p>内部最大化的替代损失。内部最大化的目标是通过求解（5）为自然示例xi生成对抗示例x
i。我们利用常用的CE损失作为替代损失。</p>
<figure>
<img src="image-20240125211407518.png" srcset="/img/loading.gif" lazyload alt="image-20240125211407518">
<figcaption aria-hidden="true">image-20240125211407518</figcaption>
</figure>
<p>强攻击可以帮助鲁棒性。我们建议使用（强）PGD攻击来最大化正确分类和错误分类示例的CE损失，与标准对抗训练相同。</p>
<p>总体目标。基于替代损失函数，我们可以为我们提出的错误分类感知对抗训练（MART）防御陈述最终目标函数：</p>
<figure>
<img src="image-20240125211526831.png" srcset="/img/loading.gif" lazyload alt="image-20240125211526831">
<figcaption aria-hidden="true">image-20240125211526831</figcaption>
</figure>
<figure>
<img src="image-20240125211541264.png" srcset="/img/loading.gif" lazyload alt="image-20240125211541264">
<figcaption aria-hidden="true">image-20240125211541264</figcaption>
</figure>
<p>这里的对抗性示例是由（10）生成的，λ是一个可调的缩放参数，用于平衡最终损失的两个部分，并且对于所有训练示例都是固定的。</p>
<h2 id="与现有工程的关系">4.2 与现有工程的关系</h2>
<p>在本节中，我们简要讨论了我们的MART和现有防御方法之间的差异，包括标准对抗训练（标准）（Madry等人，2018）、logit配对方法（Kannan等人，2018）、最大边际对抗训练（MMA）（Ding等人，2018）和TRADES（Zhang
et al.，2019年）。</p>
<figure>
<img src="image-20240125211738884.png" srcset="/img/loading.gif" lazyload alt="image-20240125211738884">
<figcaption aria-hidden="true">image-20240125211738884</figcaption>
</figure>
<p>对抗性示例x由（10）生成，用于除TRADES和MMA之外的所有防御方法。TRADES中的对抗性示例是通过最大化其正则化项（KL-发散）来生成的，而MMA中的对抗性示例是通过用不同的扰动极限求解（10）来生成的。</p>
<p>标准算法的设计目标是最小化标准对抗损失，即对抗例子的交叉熵损失。Logit配对方法由对抗Logit配对(ALP)和干净Logit配对(CLP)组成，引入了一个正则化术语，包含自然例子和它们的对抗对手。TRADES的目标函数也是自然损失和正则化项的线性组合，其输出概率对应于自然例子和使用KL发散的对抗样本。这些算法都不能区分错误分类的例子和正确分类的例子。</p>
<p>最相关的工作是MMA，它提出对正确分类的例子使用最大边际优化，而对误分类的例子保持优化不变。具体来说，对于正确分类的例子，MMA在对抗例子上采用交叉熵损失，交叉熵损失是用依赖于例子的扰动极限求解(10)生成的。对于误分类的例子，MMA直接对自然例子应用交叉熵损失。</p>
<p>我们的MART与MMA在以下几个方面的不同:(1)MMA对训练数据中的误分类例子进行了硬决策，而MART对训练数据采用了基于相应输出概率(p(ˆx，θ))的软决策方案，这可以在训练过程中共同学习;(2)对于正确分类的例子，MMA对不同扰动极限的对抗性例子采用交叉熵损失，而MART对相同扰动极限的对抗性例子采用提出的BCE损失;(3)对于误分类的例子，MMA采用了自然例子的交叉熵损失，而MART则采用了包括自然例子和对抗例子的正则化对抗损失。由于这些差异，我们稍后会证明MART在实验中优于MMA。</p>
<h2 id="带有未标记数据的半监督扩展">4.3 带有未标记数据的半监督扩展</h2>
<p>最近的研究表明，带有额外未标记数据的半监督学习可以提高对抗鲁棒性。具体来说，这些半监督学习方法中的训练损失函数通常定义为有监督损失(有标记数据的损失)和无监督损失(无标记数据的损失)的加权和，即:</p>
<figure>
<img src="image-20240125214955611.png" srcset="/img/loading.gif" lazyload alt="image-20240125214955611">
<figcaption aria-hidden="true">image-20240125214955611</figcaption>
</figure>
<p>γ &gt;
0为无监督损失的权重。无监督损失函数Lunsup(θ)有多种选择，导致不同的防御方法，其中最有效的防御方法是UAT++。特别是，UAT++首先在有标签的数据上训练一个自然模型，然后使用这个模型为没有标签的数据生成伪标签。</p>
<p>此外，给定一个训练数据(x,
y)(可以是标记数据，也可以是未标记数据)，UAT++中采用的监督和非监督丢失函数被定义为</p>
<figure>
<img src="image-20240125215727227.png" srcset="/img/loading.gif" lazyload alt="image-20240125215727227">
<figcaption aria-hidden="true">image-20240125215727227</figcaption>
</figure>
<p>这里λ是一个可调的超参数。在一项并行工作中也提出了类似的想法(Carmon等人，2019)，导致了另一种半监督防御方法，称为RST。RST的第一阶段也是通过对已标记数据训练自然模型来为未标记数据生成伪标记。然后在第二阶段，RST使用TRADES
loss来训练基于有标签和无标签数据的鲁棒模型，即给定一个训练数据(x, y)，
RST中采用的监督和无监督损失函数定义为</p>
<figure>
<img src="image-20240125215950845.png" srcset="/img/loading.gif" lazyload alt="image-20240125215950845">
<figcaption aria-hidden="true">image-20240125215950845</figcaption>
</figure>
<p>正如我们在图1(b)和下面的实验部分所指出的，最大化技术对鲁棒性的影响可以忽略不计。因此，uat++和RST之间的主要区别是最小化的目标函数。考虑到MART也是一个目标函数，因此可以很容易地将其与带有未标记数据的半监督学习结合起来。根据RST，我们提出以下MART的半监督版本:</p>
<figure>
<img src="image-20240125220046653.png" srcset="/img/loading.gif" lazyload alt="image-20240125220046653">
<figcaption aria-hidden="true">image-20240125220046653</figcaption>
</figure>
<p>有监督和无监督损失函数定义如下:</p>
<figure>
<img src="image-20240125220157874.png" srcset="/img/loading.gif" lazyload alt="image-20240125220157874">
<figcaption aria-hidden="true">image-20240125220157874</figcaption>
</figure>
<p>Ssup和Sunsup分别表示有标记数据和无标记数据的集合。</p>
<h1 id="实验">5 实验</h1>
<p>在本节中，我们首先进行了一组实验，以提供一个全面的理解我们提出的防御MART，然后评估其在白盒和黑盒设置的基准数据集上的鲁棒性。最后，我们对最先进的鲁棒性进行基准测试，并探索使用未标记数据来进一步改进。</p>
<h2 id="理解mart">5.1 理解MART</h2>
<p>在这里，我们从4个不同的角度来研究MART:(1)去除MART损失函数的成分，(2)替换MART损失函数的成分，(3)对一定比例的训练数据的误分类感知损失，(4)对正则化参数λ的敏感性。</p>
<p>实验设置。我们用CIFAR-10上的不同MART变体训练ResNet-18 (He et al.，
2016) (Krizhevsky &amp; Hinton,
2009)。所有模型都使用动量为0.9的SGD进行训练，权值衰减为2 ×
10−4，初始学习速率为0.1，即在75和90
epoch时除以10。所有的自然图像都归一化为[0,1]，简单的数据增强包括4像素填充，32
× 32随机裁剪和随机水平翻转。最大摄动= 8/255，参数λ =
6。训练攻击为随机启动，步长/4的PGD10，测试攻击为随机启动，步长/10的PGD20。</p>
<p>拆卸MART组件。回顾(11)中MART的目标函数，损失函数中有三个项:BCE、KL和1−p§。如图2(a)所示，去除1−p或KL或同时去除都会导致鲁棒性显著下降。特别地，我们发现软决策项1−p在整个训练过程中具有持续的鲁棒性提高，而KL项可以帮助缓解训练后期(80个时代之后)的过拟合。当两项结合在一起时，它们大大提高了最终的鲁棒性，而不会引起过拟合。</p>
<figure>
<img src="image-20240125233252737.png" srcset="/img/loading.gif" lazyload alt="image-20240125233252737">
<figcaption aria-hidden="true">image-20240125233252737</figcaption>
</figure>
<p>更换MART的部件。正如我们在图2(b)中所示，当BCE组件被CE项取代或在自然样本xnat)上重新定义时，最终的鲁棒性会大幅下降。这表明，用CE学习而不是我们提出的BCE学习存在学习不足的问题，在整个训练过程中鲁棒性较低。另一方面，在自然例子上使用BCE学习在后期表现出严重的过拟合(实绿线)。在对抗min-max框架的内部最大化(实线红线)中，我们没有观察到KL替代CE的任何好处，这与图1(b)一致。</p>
<figure>
<img src="image-20240125233456732.png" srcset="/img/loading.gif" lazyload alt="image-20240125233456732">
<figcaption aria-hidden="true">image-20240125233456732</figcaption>
</figure>
<p>训练数据消融。在这里，我们展示了我们提出的错误分类感知正则化(例如，(11)中的KL·(1−p)项)对训练数据的最终鲁棒性的贡献。具体来说，我们逐渐增加使用本文提出的误分类感知正则化项训练的训练示例的比例，并在图2(c)中显示出相应的鲁棒性。使用提议的正则化的训练例子是随机选择的，并且BCE项仍然在所有训练(对抗)例子上定义。可以看出，当正则化应用于更多的数据时，鲁棒性可以稳步提高。这验证了对正确分类和错误分类的例子进行区分的优越性。</p>
<figure>
<img src="image-20240125234045278.png" srcset="/img/loading.gif" lazyload alt="image-20240125234045278">
<figcaption aria-hidden="true">image-20240125234045278</figcaption>
</figure>
<p>对正则化参数λ的敏感性。我们进一步研究了(11)中定义的MART目标函数中控制正则化强度的参数λ。我们还测试了TRADES的正则化参数λ(请参见表1)。对于不同的λ∈[1/2,50]，我们给出了图2(d)¶的结果。通过显式区分错误分类和正确分类的例子，MART在λ的不同选择上实现了良好的稳定性和鲁棒性，且始终比TRADES更好更稳定。</p>
<figure>
<img src="image-20240125234122216.png" srcset="/img/loading.gif" lazyload alt="image-20240125234122216">
<figcaption aria-hidden="true">image-20240125234122216</figcaption>
</figure>
<h2 id="稳健性评价和分析">5.2 稳健性评价和分析</h2>
<p>在这一部分中，我们评估了MART在MNIST (LeCun et al.， 1998)和cifar10
(Krizhevsky &amp; Hinton,
2009)数据集上对各种白盒和黑盒攻击的鲁棒性。</p>
<p>基线。(1)Standard (Madry et al.， 2018);(2)
MMA(丁等，2019);(3)动态(Wang et al.， 2019);(4) trading (Zhang et al.，
2019)。我们只比较对抗性训练的变体，因为它们是迄今为止最有效的防御(Athalye等人，2018年)。</p>
<p>防御设置。对于MNIST，所有的防御模型都建立在一个4层CNN上，使用动量为0.9的SGD进行训练。初始学习率为0.01，并在第20和40
epoch除以10。对于cifar10，我们使用动量为0.9的SGD，权重衰减为3.5 ×
10−3，初始学习速率为0.01，即在第75和90
epoch除以10。训练攻击也是随机启动，步长/4的PGD10。MNIST的扰动极限=
0.3,CIFAR-10的扰动极限= 8/255。对于MART，我们设λ =
5。基线的超参数按照原始论文配置:MMA的最大边际设置为0.45 (MNIST)或12/255
(CIFAR-10)， Dynamic的最大标准值cmax = 0.5, TRADES的λ = 4。</p>
<p>白盒鲁棒性。对于MNIST和cifar10，我们评估了所有防御模型对三种攻击的鲁棒性:FGSM、PGD20(20步PGD，步长/10)和CW∞(PGD优化后的L∞版CW∞)。所有攻击都可以完全访问模型参数，并受到相同的摄动限制。表2报告了所有防御模型的白盒鲁棒性，其中“Natural”表示自然测试图像的准确性。我们提出的防御MART对MNIST和CIFAR-10的所有三种类型的攻击都具有最佳的鲁棒性。与MNIST相比，MART相对于其他基线的鲁棒性改进在CIFAR-10上更为显著。这是因为在CIFAR-10上进行对抗训练是一个更具挑战性的问题，在训练过程中可能会出现更多的误分类例子，而MART由于在(11)中的正则化项，可以更好地处理这些误分类的例子。</p>
<p>注意MART的鲁棒性改进不是由所谓的“模糊梯度”引起的(Athalye et al.，
2018)。这可以通过两个现象来验证:(1)强测试攻击(如PGD20)比弱测试攻击(如FGSM)有更高的成功率(更低的准确率)，(2)白盒测试攻击比黑盒测试攻击有更高的成功率(表2和表3的比较)。此外，我们使用无梯度攻击SPSA进行了额外的检查(Uesato
et al.，
2018)。SPSA攻击并没有比PGD等基于梯度的攻击获得更低的准确性，这证实了MART训练模型的鲁棒性不是由于梯度掩蔽。</p>
<figure>
<img src="image-20240126090717897.png" srcset="/img/loading.gif" lazyload alt="image-20240126090717897">
<figcaption aria-hidden="true">image-20240126090717897</figcaption>
</figure>
<p>黑盒鲁棒性。黑盒测试攻击是通过攻击具有防御模型(MNIST)或更复杂的ResNet-50模型(ciarc
-10)的体系结构的代理模型，从自然测试图像中构建而成的。在原始训练集上，代理模型和防御模型分别进行训练。这里使用的攻击方法有:FGSM、PGD10、PGD20、CW∞。所有防御模型的黑盒鲁棒性如表3所示。同样，提出的防御MART比其他基线具有更高的鲁棒性。与白盒结果相比，所有防御方法对黑盒攻击都具有更好的鲁棒性，甚至接近于自然精度。这表明对抗训练确实是一种非常实用的选择，在这种情况下，目标模型可以对潜在的攻击者保密。还观察到，对于CW∞等强攻击的鲁棒性高于FGSM等弱攻击，表明强攻击的可移动性低于弱攻击。</p>
<figure>
<img src="image-20240126090724544.png" srcset="/img/loading.gif" lazyload alt="image-20240126090724544">
<figcaption aria-hidden="true">image-20240126090724544</figcaption>
</figure>
<h2 id="最先进的稳健性基准">5.3 最先进的稳健性基准</h2>
<p>在这一部分中，我们在大容量网络WideResNet (Zagoruyko &amp; Komodakis,
2016)上进行了更多的实验，以对最先进的鲁棒性进行基准测试，并探索使用未标记数据来进一步增强鲁棒性。</p>
<p>WideResNet上的性能。我们使用WideResNet-34-10(深度34，宽度10)来探索我们所提出的MART防御方法的全部功能，并在CIFAR-10上测试了最先进的鲁棒性。在4.1相同的设置下，测试了所有防御模型对白盒FGSM、PGD20、PGD100和CW∞攻击的鲁棒性。我们在表4中报告了训练过程中获得的最佳和最后一个epoch模型的鲁棒性。对于针对每次攻击的每种防御方法，“最佳”指的是在不同检查点上达到的最高稳健性。具体来说，对于FGSM攻击，所有防御方法都在上一个epoch找到最优模型(如“best”也是“last”)，而对于PGD20、PGD100和CW∞攻击，则在第一次学习速率衰减后的epoch(即epoch
76)找到最优模型。我们提出的MART在最佳模型和最后一个epoch模型的鲁棒性方面都优于所有基线方法。特别是在最常见的比较设置下(针对针对CIFAR-10的PGD20攻击)，上一个epoch模型的MART比Standard提高了∼8%，甚至比TRADES提高了∼4%。在最佳历元模型结果中也观察到类似的改进趋势。考虑到针对所有攻击的最坏情况下的准确性，MART仍然比Standard和TRADES分别获得了~
6%和~ 3.5%的鲁棒性改进。</p>
<figure>
<img src="image-20240126091031406.png" srcset="/img/loading.gif" lazyload alt="image-20240126091031406">
<figcaption aria-hidden="true">image-20240126091031406</figcaption>
</figure>
<p>使用额外的未标记数据进行扩充。在这里，我们评估了所提出的MART的半监督版本，表明它也可以受益于额外的未标记数据，并获得更好的鲁棒性。按照UAT++
(Uesato et al.， 2019)和RST (Carmon et al.，
2019)中的精确设置，我们分别在WideResNet-34-8和WideResNet-28-10和PGD20
(FGSM20)上比较了MART和它们的鲁棒性(在他们论文中报道的相同设置中)。该数据集是CIFAR-10，有100K和500K未标记数据，提取自8000万张微小图像数据集(Torralba等人，2008年)。如表5所示，我们建议的防御MART也可以从未标记的数据中受益，并进一步改进了UAT++和RST防御。这再次验证了区分误分类和正确分类的例子对提高鲁棒性的好处，进一步证明了本文方法的优越性。</p>
<figure>
<img src="image-20240126091142848.png" srcset="/img/loading.gif" lazyload alt="image-20240126091142848">
<figcaption aria-hidden="true">image-20240126091142848</figcaption>
</figure>
<h1 id="代码复现">6 代码复现</h1>
<p>复现成功。</p>
<p>并且还另外写了一个标准对抗训练。</p>
<p>环境就是正常的torch，需要什么就加什么。</p>
<p>时间比较长。</p>
<p>已经上传到github中。</p>
<h1 id="想法">8 想法</h1>
<h2 id="可改进点">8.1 可改进点</h2>
<p>1 介绍部分，将pgd10，kl这两种方法叠加到一块。</p>
<p>2
介绍部分，只探讨了里面约束fgsm，可以加一些新方法，同理外面的约束也可以采用其他约束。</p>
<p>3
作者自己在结论中提出，可能已经做了：在未来，我们计划在最近提出的认证/可证明的鲁棒性框架中研究正确分类/错误分类训练示例的差异化效果（Cohen
et al.，2019;
Salman等人，2019年），并探索培训示例的差异化带来的潜在改进。</p>
<p>4
只探讨了分类正确和错误数目相同的情况，但是没有讨论不同的情况，而且最小化的目标可以因此添加更改系数，以及如何更改系数会更好。</p>
<p>5
4算法部分，提到使用BCE来代替CE，可以考虑其他算法来代替BCE，以及其他的可替代的，当然论文中也说了。</p>
<h2 id="问题">8.2 问题</h2>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E8%AE%BA%E6%96%87%E8%AF%BB%E5%90%8E%E6%80%BB%E7%BB%93/" class="category-chain-item">论文读后总结</a>
  
  
    <span>></span>
    
  <a href="/categories/%E8%AE%BA%E6%96%87%E8%AF%BB%E5%90%8E%E6%80%BB%E7%BB%93/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/" class="category-chain-item">对抗样本</a>
  
  
    <span>></span>
    
  <a href="/categories/%E8%AE%BA%E6%96%87%E8%AF%BB%E5%90%8E%E6%80%BB%E7%BB%93/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/" class="category-chain-item">对抗训练</a>
  
  

  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E8%AE%BA%E6%96%87%E8%AF%BB%E5%90%8E%E6%80%BB%E7%BB%93/" class="print-no-link">#论文读后总结</a>
      
        <a href="/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/" class="print-no-link">#对抗样本</a>
      
        <a href="/tags/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/" class="print-no-link">#对抗训练</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>IMPROVING ADVERSARIAL ROBUSTNESS REQUIRES REVISITING MISCLASSIFIED EXAMPLES</div>
      <div>http://example.com/2024/01/25/IMPROVING-ADVERSARIAL-ROBUSTNESS-REQUIRES-REVISITING-MISCLASSIFIED-EXAMPLES/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Guoxin</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年1月25日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/01/30/AdvDO%20Realistic%20Adversarial%20Attacks%20for%20Trajectory%20Prediction/" title="AdvDO Realistic Adversarial Attacks for Trajectory Prediction">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">AdvDO Realistic Adversarial Attacks for Trajectory Prediction</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/01/18/Vehicle-trajectory-prediction-works-but-not-everywhere/" title="Vehicle trajectory prediction works, but not everywhere">
                        <span class="hidden-mobile">Vehicle trajectory prediction works, but not everywhere</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
